"""
Keyword Analyzer - ĞĞ½Ğ°Ğ»Ñ–Ğ· ĞºĞ»ÑÑ‡Ğ¾Ğ²Ğ¸Ñ… ÑĞ»Ñ–Ğ² Ñ‚Ğ° Ñ‚Ñ€ĞµĞ½Ğ´Ñ–Ğ²
Ğ¥Ğ¼Ğ°Ñ€Ğ¸ ÑĞ»Ñ–Ğ² Ñ‚Ğ° ÑĞµĞ½Ñ‚Ğ¸Ğ¼ĞµĞ½Ñ‚-Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·
"""
import re
import logging
from typing import Dict, List, Any, Optional
from collections import Counter
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)


class KeywordAnalyzer:
    """ĞĞ½Ğ°Ğ»Ñ–Ğ· Ñ‚Ğ° Ğ²Ñ–Ğ·ÑƒĞ°Ğ»Ñ–Ğ·Ğ°Ñ†Ñ–Ñ ĞºĞ»ÑÑ‡Ğ¾Ğ²Ğ¸Ñ… ÑĞ»Ñ–Ğ²"""
    
    STOP_WORDS_UA = {
        'Ñ–', 'Ñ‚Ğ°', 'Ğ²', 'Ğ½Ğ°', 'Ğ·', 'Ñƒ', 'Ğ´Ğ¾', 'Ğ²Ñ–Ğ´', 'Ğ·Ğ°', 'Ğ¿Ñ€Ğ¾',
        'Ñ‰Ğ¾', 'ÑĞº', 'Ñ†Ğµ', 'Ñ‚Ğ¾Ğ¹', 'Ñ†Ñ', 'Ğ´Ğ»Ñ', 'Ğ¿Ñ€Ğ¸', 'Ğ¿Ğ¾', 'Ğ½Ğµ',
        'Ñ‚Ğ°Ğº', 'Ğ°Ğ»Ğµ', 'Ğ°Ğ±Ğ¾', 'Ñ‡Ğ¸', 'Ğ½Ñ–', 'Ğ²Ñ–Ğ½', 'Ğ²Ğ¾Ğ½Ğ°', 'Ğ²Ğ¾Ğ½Ğ¾',
        'Ğ¼Ğ¸', 'Ğ²Ğ¸', 'Ğ²Ğ¾Ğ½Ğ¸', 'Ñ—Ñ…', 'Ğ¹Ğ¾Ğ³Ğ¾', 'Ñ—Ñ—', 'Ğ½Ğ°Ñˆ', 'Ğ²Ğ°Ñˆ',
        'ÑĞºĞ¸Ğ¹', 'ÑĞºĞ°', 'ÑĞºĞµ', 'ÑĞºÑ–', 'Ñ†ĞµĞ¹', 'Ñ†Ñ', 'Ñ†Ğµ', 'Ñ†Ñ–',
        'Ñ‚Ğ¾Ğ¹', 'Ñ‚Ğ°', 'Ñ‚Ğµ', 'Ñ‚Ñ–', 'Ğ²ÑĞµ', 'Ğ²ÑÑ–', 'ĞºĞ¾Ğ¶ĞµĞ½', 'ĞºĞ¾Ğ¶Ğ½Ğ°'
    }
    
    STOP_WORDS_RU = {
        'Ğ¸', 'Ğ²', 'Ğ²Ğ¾', 'Ğ½Ğµ', 'Ñ‡Ñ‚Ğ¾', 'Ğ¾Ğ½', 'Ğ½Ğ°', 'Ñ', 'Ñ', 'ÑĞ¾',
        'ĞºĞ°Ğº', 'Ğ°', 'Ñ‚Ğ¾', 'Ğ²ÑĞµ', 'Ğ¾Ğ½Ğ°', 'Ñ‚Ğ°Ğº', 'ĞµĞ³Ğ¾', 'Ğ½Ğ¾', 'Ğ´Ğ°',
        'Ñ‚Ñ‹', 'Ğº', 'Ñƒ', 'Ğ¶Ğµ', 'Ğ²Ñ‹', 'Ğ·Ğ°', 'Ğ±Ñ‹', 'Ğ¿Ğ¾', 'Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾',
        'ĞµÑ‘', 'Ğ¼Ğ½Ğµ', 'Ğ±Ñ‹Ğ»Ğ¾', 'Ğ²Ğ¾Ñ‚', 'Ğ¾Ñ‚', 'Ğ¼ĞµĞ½Ñ', 'ĞµÑ‰Ñ‘', 'Ğ½ĞµÑ‚',
        'Ğ¾', 'Ğ¸Ğ·', 'ĞµĞ¼Ñƒ', 'Ñ‚ĞµĞ¿ĞµÑ€ÑŒ', 'ĞºĞ¾Ğ³Ğ´Ğ°', 'ÑƒĞ¶Ğµ', 'Ğ²Ğ°Ğ¼', 'Ğ½Ğ¸'
    }
    
    SENTIMENT_POSITIVE = [
        'Ğ´Ğ¾Ğ±Ñ€Ğµ', 'Ñ‡ÑƒĞ´Ğ¾Ğ²Ğ¾', 'Ğ²Ñ–Ğ´Ğ¼Ñ–Ğ½Ğ½Ğ¾', 'ÑÑƒĞ¿ĞµÑ€', 'ĞºĞ»Ğ°Ñ', 'ĞºÑ€ÑƒÑ‚Ğ¾',
        'Ğ´ÑĞºÑƒÑ', 'Ğ²Ğ´ÑÑ‡Ğ½Ğ¸Ğ¹', 'Ñ€Ğ°Ğ´Ğ¸Ğ¹', 'Ğ·Ğ°Ğ´Ğ¾Ğ²Ğ¾Ğ»ĞµĞ½Ğ¸Ğ¹', 'Ğ»ÑĞ±Ğ¾Ğ²',
        'Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾', 'Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ğ¾', 'ĞºĞ»Ğ°ÑÑ', 'ÑĞ¿Ğ°ÑĞ¸Ğ±Ğ¾', 'Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ'
    ]
    
    SENTIMENT_NEGATIVE = [
        'Ğ¿Ğ¾Ğ³Ğ°Ğ½Ğ¾', 'Ğ¶Ğ°Ñ…Ğ»Ğ¸Ğ²Ğ¾', 'Ğ¾Ğ³Ğ¸Ğ´Ğ½Ğ¾', 'Ğ½ĞµĞ½Ğ°Ğ²Ğ¸Ğ´Ğ¶Ñƒ', 'Ğ·Ğ»Ğ¸Ğ¹',
        'Ğ¿Ğ¾Ğ³Ğ°Ğ½Ğ¸Ğ¹', 'Ğ¶Ğ°Ñ…', 'Ğ±Ñ–Ğ´Ğ°', 'Ğ³Ğ¾Ñ€Ğµ', 'ÑÑ‚Ñ€Ğ°Ñ…', 'Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°',
        'Ğ¿Ğ»Ğ¾Ñ…Ğ¾', 'ÑƒĞ¶Ğ°ÑĞ½Ğ¾', 'Ğ½ĞµĞ½Ğ°Ğ²Ğ¸Ğ¶Ñƒ', 'Ğ·Ğ»Ğ¾Ğ¹', 'Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°'
    ]
    
    def __init__(self):
        self.all_stop_words = self.STOP_WORDS_UA | self.STOP_WORDS_RU
    
    def analyze_text(self, text: str) -> Dict[str, Any]:
        """ĞŸĞ¾Ğ²Ğ½Ğ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ· Ñ‚ĞµĞºÑÑ‚Ñƒ"""
        words = self._extract_words(text)
        filtered = self._filter_words(words)
        
        return {
            'total_words': len(words),
            'unique_words': len(set(words)),
            'filtered_words': len(filtered),
            'word_frequency': self._get_word_frequency(filtered),
            'top_keywords': self._get_top_keywords(filtered, 20),
            'sentiment': self._analyze_sentiment(text),
            'language': self._detect_language(text),
            'readability': self._calculate_readability(text)
        }
    
    def analyze_messages(self, messages: List[Dict]) -> Dict[str, Any]:
        """ĞĞ½Ğ°Ğ»Ñ–Ğ· ÑĞ¿Ğ¸ÑĞºÑƒ Ğ¿Ğ¾Ğ²Ñ–Ğ´Ğ¾Ğ¼Ğ»ĞµĞ½ÑŒ"""
        all_text = ' '.join(msg.get('text', '') for msg in messages)
        
        basic_analysis = self.analyze_text(all_text)
        
        time_analysis = self._analyze_time_distribution(messages)
        trending = self._identify_trending(messages)
        
        return {
            **basic_analysis,
            'message_count': len(messages),
            'time_distribution': time_analysis,
            'trending_words': trending
        }
    
    def _extract_words(self, text: str) -> List[str]:
        """Ğ’Ğ¸Ñ‚ÑĞ³ ÑĞ»Ñ–Ğ² Ğ· Ñ‚ĞµĞºÑÑ‚Ñƒ"""
        text = text.lower()
        text = re.sub(r'https?://\S+', '', text)
        text = re.sub(r'@\w+', '', text)
        text = re.sub(r'#\w+', '', text)
        words = re.findall(r'[\w\']+', text)
        return [w for w in words if len(w) > 2 and not w.isdigit()]
    
    def _filter_words(self, words: List[str]) -> List[str]:
        """Ğ¤Ñ–Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ñ–Ñ ÑÑ‚Ğ¾Ğ¿-ÑĞ»Ñ–Ğ²"""
        return [w for w in words if w not in self.all_stop_words]
    
    def _get_word_frequency(self, words: List[str]) -> Dict[str, int]:
        """ĞŸÑ–Ğ´Ñ€Ğ°Ñ…ÑƒĞ½Ğ¾Ğº Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ¸ ÑĞ»Ñ–Ğ²"""
        return dict(Counter(words))
    
    def _get_top_keywords(self, words: List[str], n: int = 20) -> List[Dict]:
        """ĞÑ‚Ñ€Ğ¸Ğ¼Ğ°Ğ½Ğ½Ñ Ñ‚Ğ¾Ğ¿ ĞºĞ»ÑÑ‡Ğ¾Ğ²Ğ¸Ñ… ÑĞ»Ñ–Ğ²"""
        counter = Counter(words)
        total = len(words)
        
        return [
            {
                'word': word,
                'count': count,
                'percentage': round(count / total * 100, 2) if total > 0 else 0
            }
            for word, count in counter.most_common(n)
        ]
    
    def _analyze_sentiment(self, text: str) -> Dict[str, Any]:
        """ĞĞ½Ğ°Ğ»Ñ–Ğ· ÑĞµĞ½Ñ‚Ğ¸Ğ¼ĞµĞ½Ñ‚Ñƒ Ñ‚ĞµĞºÑÑ‚Ñƒ"""
        text_lower = text.lower()
        
        positive_count = sum(1 for word in self.SENTIMENT_POSITIVE if word in text_lower)
        negative_count = sum(1 for word in self.SENTIMENT_NEGATIVE if word in text_lower)
        
        total_sentiment = positive_count + negative_count
        if total_sentiment == 0:
            score = 0
            label = 'neutral'
        else:
            score = (positive_count - negative_count) / total_sentiment
            if score > 0.2:
                label = 'positive'
            elif score < -0.2:
                label = 'negative'
            else:
                label = 'neutral'
        
        return {
            'score': round(score, 2),
            'label': label,
            'positive_words': positive_count,
            'negative_words': negative_count
        }
    
    def _detect_language(self, text: str) -> str:
        """Ğ’Ğ¸Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ñ Ğ¼Ğ¾Ğ²Ğ¸ Ñ‚ĞµĞºÑÑ‚Ñƒ"""
        ua_chars = len(re.findall(r'[Ñ–Ñ—Ñ”Ò‘]', text.lower()))
        ru_chars = len(re.findall(r'[Ñ‹ÑÑŠÑ‘]', text.lower()))
        
        if ua_chars > ru_chars:
            return 'uk'
        elif ru_chars > ua_chars:
            return 'ru'
        
        cyrillic = len(re.findall(r'[Ğ°-ÑĞ-Ğ¯]', text))
        latin = len(re.findall(r'[a-zA-Z]', text))
        
        if cyrillic > latin:
            return 'uk'
        return 'en'
    
    def _calculate_readability(self, text: str) -> Dict[str, Any]:
        """Ğ Ğ¾Ğ·Ñ€Ğ°Ñ…ÑƒĞ½Ğ¾Ğº Ñ‡Ğ¸Ñ‚Ğ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ñ–"""
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        words = self._extract_words(text)
        
        if not sentences or not words:
            return {'score': 0, 'level': 'unknown'}
        
        avg_sentence_length = len(words) / len(sentences)
        avg_word_length = sum(len(w) for w in words) / len(words)
        
        score = 100 - (avg_sentence_length * 2) - (avg_word_length * 10)
        score = max(0, min(100, score))
        
        if score >= 70:
            level = 'easy'
        elif score >= 40:
            level = 'medium'
        else:
            level = 'hard'
        
        return {
            'score': round(score, 1),
            'level': level,
            'avg_sentence_length': round(avg_sentence_length, 1),
            'avg_word_length': round(avg_word_length, 1)
        }
    
    def _analyze_time_distribution(self, messages: List[Dict]) -> Dict[str, int]:
        """ĞĞ½Ğ°Ğ»Ñ–Ğ· Ñ€Ğ¾Ğ·Ğ¿Ğ¾Ğ´Ñ–Ğ»Ñƒ Ğ·Ğ° Ñ‡Ğ°ÑĞ¾Ğ¼"""
        hour_counts = Counter()
        
        for msg in messages:
            timestamp = msg.get('timestamp')
            if timestamp:
                if isinstance(timestamp, str):
                    try:
                        timestamp = datetime.fromisoformat(timestamp)
                    except:
                        continue
                if isinstance(timestamp, datetime):
                    hour_counts[timestamp.hour] += 1
        
        return dict(sorted(hour_counts.items()))
    
    def _identify_trending(self, messages: List[Dict]) -> List[Dict]:
        """Ğ†Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ñ–ĞºĞ°Ñ†Ñ–Ñ Ñ‚Ñ€ĞµĞ½Ğ´Ğ¾Ğ²Ğ¸Ñ… ÑĞ»Ñ–Ğ²"""
        if len(messages) < 10:
            return []
        
        mid_point = len(messages) // 2
        
        older = messages[:mid_point]
        newer = messages[mid_point:]
        
        older_words = Counter(self._filter_words(self._extract_words(
            ' '.join(m.get('text', '') for m in older)
        )))
        newer_words = Counter(self._filter_words(self._extract_words(
            ' '.join(m.get('text', '') for m in newer)
        )))
        
        trending = []
        for word, new_count in newer_words.most_common(50):
            old_count = older_words.get(word, 0)
            if old_count == 0:
                growth = new_count * 100
            else:
                growth = (new_count - old_count) / old_count * 100
            
            if growth > 50:
                trending.append({
                    'word': word,
                    'new_count': new_count,
                    'old_count': old_count,
                    'growth': round(growth, 1)
                })
        
        return sorted(trending, key=lambda x: x['growth'], reverse=True)[:10]
    
    def format_analysis_report(self, analysis: Dict) -> str:
        """Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ÑƒĞ²Ğ°Ğ½Ğ½Ñ Ğ·Ğ²Ñ–Ñ‚Ñƒ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·Ñƒ"""
        report = [
            "<b>ğŸ“Š ĞĞĞĞ›Ğ†Ğ— ĞšĞ›Ğ®Ğ§ĞĞ’Ğ˜Ğ¥ Ğ¡Ğ›Ğ†Ğ’</b>",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
            f"Ğ’ÑÑŒĞ¾Ğ³Ğ¾ ÑĞ»Ñ–Ğ²: {analysis['total_words']}",
            f"Ğ£Ğ½Ñ–ĞºĞ°Ğ»ÑŒĞ½Ğ¸Ñ…: {analysis['unique_words']}",
            f"ĞœĞ¾Ğ²Ğ°: {analysis['language'].upper()}",
            ""
        ]
        
        sentiment = analysis.get('sentiment', {})
        emoji = {'positive': 'ğŸ˜Š', 'negative': 'ğŸ˜', 'neutral': 'ğŸ˜'}
        report.append(f"<b>Ğ¡ĞµĞ½Ñ‚Ğ¸Ğ¼ĞµĞ½Ñ‚:</b> {emoji.get(sentiment.get('label', 'neutral'), 'â“')} {sentiment.get('label', 'N/A')}")
        report.append(f"ĞÑ†Ñ–Ğ½ĞºĞ°: {sentiment.get('score', 0)}")
        report.append("")
        
        report.append("<b>Ğ¢ĞĞŸ-10 ÑĞ»Ñ–Ğ²:</b>")
        for kw in analysis.get('top_keywords', [])[:10]:
            report.append(f"â”œ {kw['word']}: {kw['count']} ({kw['percentage']}%)")
        
        readability = analysis.get('readability', {})
        if readability:
            report.append("")
            report.append(f"<b>Ğ§Ğ¸Ñ‚Ğ°Ğ±ĞµĞ»ÑŒĞ½Ñ–ÑÑ‚ÑŒ:</b> {readability.get('level', 'N/A')}")
            report.append(f"â”œ Ğ‘Ğ°Ğ»: {readability.get('score', 0)}/100")
            report.append(f"â”œ Ğ¡ĞµÑ€. Ğ´Ğ¾Ğ²Ğ¶Ğ¸Ğ½Ğ° Ñ€ĞµÑ‡ĞµĞ½Ğ½Ñ: {readability.get('avg_sentence_length', 0)}")
            report.append(f"â”” Ğ¡ĞµÑ€. Ğ´Ğ¾Ğ²Ğ¶Ğ¸Ğ½Ğ° ÑĞ»Ğ¾Ğ²Ğ°: {readability.get('avg_word_length', 0)}")
        
        trending = analysis.get('trending_words', [])
        if trending:
            report.append("")
            report.append("<b>ğŸ“ˆ Ğ¢Ñ€ĞµĞ½Ğ´Ğ¸:</b>")
            for tw in trending[:5]:
                report.append(f"â”œ {tw['word']}: +{tw['growth']}%")
        
        return '\n'.join(report)


keyword_analyzer = KeywordAnalyzer()
